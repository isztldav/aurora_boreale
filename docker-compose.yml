services:
  db:
    image: postgres:15
    container_name: dashboard-db
    environment:
      POSTGRES_USER: dashboard
      POSTGRES_PASSWORD: dashboard
      POSTGRES_DB: dashboard
    volumes:
      - pgdata:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U $$POSTGRES_USER -d $$POSTGRES_DB"]
      interval: 10s
      timeout: 5s
      retries: 10
    networks:
      - internal

  web:
    build:
      context: .
      dockerfile: src/dashboard/Dockerfile
    container_name: dashboard-web
    working_dir: /app
    # Mount the app source and bind host ./runs for TensorBoard logs/checkpoints
    volumes:
      - ./:/app
      - ./runs:/app/runs
    environment:
      # Point the app to Postgres in the internal network
      DASHBOARD_DB_URL: postgresql+psycopg2://dashboard:dashboard@db:5432/dashboard
      # Optional: tune TensorBoard mount lifecycle in the web app
      TB_IDLE_TIMEOUT: "600"       # seconds
      TB_SWEEP_INTERVAL: "30"      # seconds
      PYTHONUNBUFFERED: "1"
      PYTHONPATH: /app/src
    command: >
      bash -lc "uvicorn src.dashboard.app:app --host 0.0.0.0 --port 8000"
    depends_on:
      db:
        condition: service_healthy
    ports:
      - "8000:8000"
    networks:
      - internal

  agent:
    # Build an image that pre-installs heavy CUDA/PyTorch deps
    build:
      context: .
      dockerfile: src/agent/Dockerfile
    container_name: training-agent
    working_dir: /app
    # Mount the app source and the same host bind for writing TB logs
    volumes:
      - ./:/app
      - ./runs:/app/runs
      # Host datasets folder mounted inside the agent container
      - ./datasets:/app/datasets
    environment:
      DASHBOARD_DB_URL: postgresql+psycopg2://dashboard:dashboard@db:5432/dashboard
      NVIDIA_VISIBLE_DEVICES: all
      PYTHONUNBUFFERED: "1"
      # Where datasets are mounted inside the container
      DATASETS_DIR: /app/datasets
      # GPU index this container will serve (one agent per GPU)
      GPU_INDEX: "0"
      PYTHONPATH: /app/src
    # Run the agent directly; deps are already installed in the image
    command: bash -lc "python -m agent.server --gpu-index $$GPU_INDEX"
    # Increase shared memory for PyTorch DataLoader workers
    shm_size: "2gb"
    depends_on:
      db:
        condition: service_healthy
    # Request all GPUs from the host (Compose v2+)
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
#              count: 1
              device_ids: ["${GPU_INDEX:-0}"]
              capabilities: [gpu]
    networks:
      - internal

  ui:
    image: node:24-alpine
    container_name: dashboard-ui
    working_dir: /app
    volumes:
      - ./web_ui:/app
    environment:
      # Use reverse-proxied relative API path; Nginx will route /api -> web:8000
      NEXT_PUBLIC_API_BASE: /api/v1
    command: sh -lc "npm install && npm run build && npm run start"
    depends_on:
      - web
    networks:
      - internal

  nginx:
    image: nginx:alpine
    container_name: dashboard-nginx
    depends_on:
      - ui
      - web
    volumes:
      - ./deploy/nginx/ui.conf:/etc/nginx/conf.d/default.conf:ro
    ports:
      - "8080:80"
    networks:
      - internal

networks:
  internal:
    driver: bridge

volumes:
  pgdata:
